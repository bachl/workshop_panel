@Article{bellExplainingFixedEffects2015,
  title = {Explaining Fixed Effects: {{Random}} Effects Modeling of Time-Series Cross-Sectional and Panel Data},
  shorttitle = {Explaining {{Fixed Effects}}},
  author = {Andrew Bell and Kelvyn Jones},
  year = {2015},
  month = {jan},
  volume = {3},
  pages = {133--153},
  publisher = {{Cambridge University Press}},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2014.7},
  abstract = {This article challenges Fixed Effects (FE) modeling as the `default' for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling\textemdash{}correlated lower-level covariates and higher-level residuals\textemdash{}is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Pl{\"u}mper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.},
  file = {/Users/markobachl/Zotero/storage/LUVEBVBQ/Bell und Jones - 2015 - Explaining Fixed Effects Random Effects Modeling .pdf;/Users/markobachl/Zotero/storage/5SXS9HQC/0334A27557D15848549120FE8ECD8D63.html},
  journal = {Political Science Research and Methods},
  language = {en},
  number = {1},
}

@Article{vaiseyWhatYouCan2017,
  title = {What You Can---and Can't---Do with Three-Wave Panel Data},
  author = {Stephen Vaisey and Andrew Miles},
  year = {2017},
  month = {jan},
  volume = {46},
  pages = {44--67},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124114547769},
  abstract = {The recent change in the general social survey (GSS) to a rotating panel design is a landmark development for social scientists. Sociological methodologists have argued that fixed-effects (FE) models are generally the best starting point for analyzing panel data because they allow analysts to control for unobserved time-constant heterogeneity. We review these treatments and demonstrate the advantages of FE models in the context of the GSS. We also show, however, that FE models have two rarely tested assumptions that can seriously bias parameter estimates when violated. We provide simple tests for these assumptions. We further demonstrate that FE models are extremely sensitive to the correct specification of temporal lags. We provide a simulation and a proof to show that the use of incorrect lags in FE models can lead to coefficients that are the opposite sign of the true parameter values.},
  file = {/Users/markobachl/Zotero/storage/SSZJI8DQ/Vaisey und Miles - 2017 - What You Can—and Can’t—Do With Three-Wave Panel Da.pdf},
  journal = {Sociological Methods \& Research},
  language = {en},
  number = {1},
}
@Article{kingHowRobustStandard2015,
  title = {How {{Robust Standard Errors Expose Methodological Problems They Do Not Fix}}, and {{What}} to {{Do About It}}},
  author = {Gary King and Margaret E. Roberts},
  year = {2015},
  volume = {23},
  pages = {159--179},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpu015},
  abstract = {``Robust standard errors'' are used in a vast array of scholarship to correct standard errors for model misspecification. However, when misspecification is bad enough to make classical and robust standard errors diverge, assuming that it is nevertheless not so bad as to bias everything else requires considerable optimism. And even if the optimism is warranted, settling for a misspecified model, with or without robust standard errors, will still bias estimators of all but a few quantities of interest. The resulting cavernous gap between theory and practice suggests that considerable gains in applied statistics may be possible. We seek to help researchers realize these gains via a more productive way to understand and use robust standard errors; a new general and easier-to-use ``generalized information matrix test'' statistic that can formally assess misspecification (based on differences between robust and classical variance estimates); and practical illustrations via simulations and real examples from published research. How robust standard errors are used needs to change, but instead of jettisoning this popular tool we show how to use it to provide effective clues about model misspecification, likely biases, and a guide to considerably more reliable, and defensible, inferences. Accompanying this article is software that implements the methods we describe.},
  file = {/Users/markobachl/Zotero/storage/ZYKSSNWN/King und Roberts - 2015 - How Robust Standard Errors Expose Methodological P.pdf;/Users/markobachl/Zotero/storage/7LQNUNFL/7E2F393B16FC6BCD6DA8C19D8D5D1F6B.html},
  journal = {Political Analysis},
  language = {en},
  number = {2},
}
@Article{keeleCausalInterpretationEstimated2019,
  title = {The Causal Interpretation of Estimated Associations in Regression Models},
  author = {Luke Keele and Randolph T. Stevenson and Felix Elwert},
  year = {2019},
  pages = {1--13},
  issn = {2049-8470},
  doi = {10.1017/psrm.2019.31},
  abstract = {A common causal identification strategy in political science is selection on observables. This strategy assumes one observes a set of covariates that is, after statistical adjustment, sufficient to make treatment status as-if random. Under adjustment methods such as matching or inverse probability weighting, coefficients for control variables are treated as nuisance parameters and are not directly estimated. This is in direct contrast to regression approaches where estimated parameters are obtained for all covariates. Analysts often find it tempting to give a causal interpretation to all the parameters in such regression models\textemdash{}indeed, such interpretations are often central to the proposed research design. In this paper, we ask when we can justify interpreting two or more coefficients in a regression model as causal parameters. We demonstrate that analysts must appeal to causal identification assumptions to give estimates causal interpretations. Under selection on observables, this task is complicated by the fact that more than one causal effect might be identified. We show how causal graphs provide a framework for clearly delineating which effects are presumed to be identified and thus merit a causal interpretation, and which are not. We conclude with a set of recommendations for how researchers should interpret estimates from regression models when causal inference is the goal.},
  file = {/Users/markobachl/Zotero/storage/PJZL8FDL/causal_interpretation_of_estimated_association.pdf},
  journal = {Political Science Research and Methods},
  keywords = {Causal inference},
}
@book{wooldridge10,
	Author = {Wooldridge, Jeffrey M},
	Date-Added = {2017-10-23 20:25:39 +0000},
	Date-Modified = {2017-10-23 20:25:40 +0000},
	Publisher = {MIT press},
	Title = {Econometric analysis of cross section and panel data},
	Year = {2010}}@Book{gelmanDataAnalysisUsing2006,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {A. Gelman and J. Hill},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  file = {/Users/markobachl/Zotero/storage/JY6IGG2X/[Andrew_Gelman,_Jennifer_Hill]_Data_Analysis_Using(Bookos.org).pdf},
  isbn = {0-521-86706-1},
}
@Article{barrRandomEffectsStructure2013,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  author = {Dale J. Barr and Roger Levy and Christoph Scheepers and Harry J. Tily},
  year = {2013},
  volume = {68},
  pages = {255--278},
  issn = {0749-596X},
  doi = {http://dx.doi.org/10.1016/j.jml.2012.11.001},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  file = {/Users/markobachl/Zotero/storage/RXZN6DLS/barr et al 2013.pdf},
  journal = {Journal of Memory and Language},
  keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,Statistics},
  number = {3},
}
@Article{matuschekBalancingTypeError2017,
  title = {Balancing {{Type I}} Error and Power in Linear Mixed Models},
  author = {Hannes Matuschek and Reinhold Kliegl and Shravan Vasishth and Harald Baayen and Douglas Bates},
  year = {2017},
  month = {jun},
  volume = {94},
  pages = {305--315},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.01.001},
  abstract = {Linear mixed-effects models have increasingly replaced mixed-model analyses of variance for statistical inference in factorial psycholinguistic experiments. Although LMMs have many advantages over ANOVA, like ANOVAs, setting them up for data analysis also requires some care. One simple option, when numerically possible, is to fit the full variance-covariance structure of random effects (the maximal model; Barr, Levy, Scheepers \& Tily, 2013), presumably to keep Type I error down to the nominal {$\alpha$} in the presence of random effects. Although it is true that fitting a model with only random intercepts may lead to higher Type I error, fitting a maximal model also has a cost: it can lead to a significant loss of power. We demonstrate this with simulations and suggest that for typical psychological and psycholinguistic data, higher power is achieved without inflating Type I error rate if a model selection criterion is used to select a random effect structure that is supported by the data.},
  file = {/Users/markobachl/Zotero/storage/T9EC8R2X/Matuschek et al. - 2017 - Balancing Type I error and power in linear mixed m.pdf;/Users/markobachl/Zotero/storage/GTIYWUMN/S0749596X17300013.html},
  journal = {Journal of Memory and Language},
  keywords = {Hypothesis testing,Linear mixed effect model,Power},
  language = {en},
}
